# **12.Create Kubernetes Cluster Step-by-Step**

## **Prepare Working Environment**

- Declare Paremeters 

    - At the bastion/ansible Server

          export ETCD_1=10.160.153.19
          export ETCD_2=10.160.153.18
          export ETCD_3=10.160.153.17

          export LB_1=10.160.153.19

          export MASTER_1=10.160.153.19
          export MASTER_2=10.160.153.18
          export MASTER_3=10.160.153.17

          export WORKER_1=10.160.153.19
          export WORKER_2=10.160.153.18
          export WORKER_3=10.160.153.17

          export NEXUS_0=10.160.153.17

          export K8S_VERSION=v1.24.1
          export Cluster_Name=1.a.dev.k8s.ymlee


- Enable ssh login and change password
    - At the bastion/ansible Server
          
          ssh-copy-id -i ~/.ssh/id_rsa.pub root$MASTER_1   // for all MASTER_1,2,3  WORKER_1,2,3
 
    - After ssh login, change password

          ssh $MASTER_1   // for all MASTER_1,2,3  WORKER_1,2,3

          passwd

          # Before Exit, run the Parameter Declaration at all Kubernetes nodes

          exit


## **Create Kubernetes Cluster with kubeadm**

1. **Create Repo File**

    - **Prerequisite**
      - The "NEXUS" server was built and ready as the repository for the Airgap Environment 
      - use Nexus YUM Group Repository
      - Proxy needed : Docker-ce, K8s.gcr.io

    1.1. Create Repo File for all Kubernetes nodes
    
    - At the bastion/ansible Server

          ssh $MASTER_1   // for all MASTER_1,2,3  WORKER_1,2,3

    - after connection, Create Repo File

          cat <<EOF > /etc/yum.repos.d/nexus.repo

          [nexus]
          name=Nexus Proxy
          baseurl=http://${NEXUS_0}:8081/repository/yum-group/
          enabled=1
          gpgcheck=0

          EOF

          cat /etc/yum.repos.d/nexus.repo


2. **Install CRI(containerd)**

    - **Prerequisite**
      - The CRI (Container Runtime Interface) for the Kubernetes Cluster : containerd


    2.1. **Create OS configuration file fo CRI(containerd)**

    - At all Kuberentes nodes

          cat <<EOF | tee /etc/modules-load.d/containerd.conf
          overlay
          br_netfilter
          EOF

          modprobe overlay
          modprobe br_netfilter

          cat <<EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf
          net.bridge.bridge-nf-call-iptables  = 1
          net.ipv4.ip_forward                 = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          EOF

          sysctl --system

    2.2. **Install CRI : containerd**

    - At all Kuberentes nodes

          yum install -y containerd.io


    2.3. **Modity Containerd Configuration File**

    - To make script set with variable value (the IP of NEXUS_0) to insert into the configuration file.
    - At any one of those Kuberentes nodes ==> at MASTER_1

          cat <<EOF> temptext_for_configtoml.txt
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
                    endpoint = ["http://${NEXUS_0}:5001"]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]
                    endpoint = ["http://${NEXUS_0}:5001"]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
                    endpoint = ["http://${NEXUS_0}:5001"]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${NEXUS_0}:5001"]
                    endpoint = ["http://${NEXUS_0}:5001"]    
          EOF

          cat temptext_for_configtoml.txt

    - Build and Open the "config.toml" file
    - At all Kuberentes nodes

          mkdir -p /etc/containerd
          containerd config default | tee /etc/containerd/config.toml

          vi /etc/containerd/config.toml

    - Cgroup setting
      - Change the Cgroup setting to "SystemdCgroup = true"
    - At the config.toml file of all Kuberentes nodes

          # Cgroup setting
          # Change the value of "SystemdCgroup" to "true"
          ...
              [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
                ...

                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options] 
                    SystemdCgroup = true    
                ...

    - Repository setting
      - Copy the Script from "temptext_for_configtoml.txt" above and insert it 
    - At the config.toml file of all Kuberentes nodess

          # Repository setting
          # The IP address of endpoint is the private IP of Nexus server          
          ...
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]            # Copied Script
                    endpoint = ["http://${NEXUS_0}:5001"]                                       # Copied Script
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]           # Copied Script
                    endpoint = ["http://${NEXUS_0}:5001"]                                       # Copied Script
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]              # Copied Script
                    endpoint = ["http://${NEXUS_0}:5001"]                                       # Copied Script
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."${NEXUS_0}:5001"]      # Copied Script
                    endpoint = ["http://${NEXUS_0}:5001"]                                       # Copied Script
                  ...
          
          # 모든 Deployment 는 yaml 내 image 주소 수정으로 Proxy Mirror (예. ${NEXUS_0}:5001)를 통해서 가능하나, 일부 오픈소스 배포 시 변경을 못하는 경우가 있는 관계로 docker, k8s, quay 등 기본적인 mirror 들은 추가해두는 것이 좋을 것으로 보임
          # 예) Prometheus 배포 시, POD 내에서 자체적으로 (yaml 이 아닌) 배포되는 container 가 있으며 quay.io 에서 이미지를 찾게끔 되어있음

    2.4 **Restart CRI**
    - At all Kuberentes nodes

          systemctl restart containerd

          systemctl enable containerd

          systemctl status containerd    


3. **Install Kubernetes software**

    - **Prerequisite**
      - The 
      
    3.1. **OS configuration**

    - At all Kuberentes nodes

          cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
          br_netfilter
          EOF

          cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
          net.bridge.bridge-nf-call-ip6tables = 1
          net.bridge.bridge-nf-call-iptables = 1
          EOF

          sysctl --system

          setenforce 0

          sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

          swapoff -a
          sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    3.2. **Install k8s core files : Kubelet, Kubeadm, Kubectl**
    - Install Kubernetes core Files
      - Kubernetes Version : 1.24.1
    - At all Kuberentes nodes

          yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=Kubernetes
          
          systemctl enable kubelet.service

    - Download image
    - At all Kuberentes nodes

          kubeadm config images pull --kubernetes-version v1.24.1


    3.3. **Edit the "k8s.conf" file**
    - Open the "k8s.conf" file
    - At all Kuberentes nodes

          vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

    - Set Cgroup Driver as "systemd" in the Kubelet Cgroup argument 
    - Insert Following at the Environment

          Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

    - Declare Kubelet Cgroup argument 
    - Append Following argument at the end

          $KUBELET_CGROUP_ARGS

          # Save the change of "k8s.conf" and exit the vi 
          :wq

    3.4 **Restart kubelet**

    - Restart the kubelet and check the status is running
    - At all Kuberentes nodes

          systemctl daemon-reload
          systemctl restart kubelet
          systemctl enable kubelet
          systemctl status kubelet -l

4. **Setup Kubernetes Master node 1**
    - to prepare initialize K8s cluster
    - **Prerequisite**
      - The external etcd cluster was built and ready 
      - All cert files of etcd copied in the bastion server under "~/files/ETCD-1/etc/etcd/pki" directory
      - The Load Balancer for Control Plane was ready
      - Check the IP address of LB_1

    4.1 **get the etcd certs**
    - Copy the etcd cert from Bastion to MASTER_1 nodes
    - at the Bastion Server 

          cd ~/files/ETCD-1/etc/etcd/pki
          
          sftp $MASTER_1 

          put ca.*
          put apiserver-etcd-client.*

          # or
          # pur ~/files/ETCD-1/etc/etcd/pki/ca.*
          # put ~/files/ETCD-1/etc/etcd/pki/apiserver-etcd-client.*

          exit

    - Move the etcd cert to right location in the MASTER_1 nodes
    - at the MASTER_1 nodes

          ssh $MASTER_1 

          mkdir -p /etc/kubernetes/pki/etcd
          mv ./ca.* /etc/kubernetes/pki/etcd
          mv ./apiserver-etcd-client.* /etc/kubernetes/pki

          ls -l /etc/kubernetes/pki

    - **Do Not Exit from MASTER_1 nodes**

    4.2 **Check the Load Balancer for the control plane is ready**
    - at the MASTER_1 nodes
    - If the MASTER_1 is restarted or reconnected, the Environment Parameter should be export again before execute 

          # nc -v LOAD_BALANCER_IP PORT

          nc -v ${LB_1} 6443

          Ctrl-C


    4.3 **Create the master configuration File for kubenetes Cluster**
    - Create the File : kubeadm-config.yaml
    - at the MASTER_1 nodes

          cat <<EOF> kubeadm-config.yaml
          apiVersion: kubeadm.k8s.io/v1beta3
          kind: InitConfiguration
          nodeRegistration:
          criSocket: "/run/containerd/containerd.sock"
          ---
          apiVersion: kubeadm.k8s.io/v1beta2
          kind: ClusterConfiguration
          clusterName: 1.a.dev.k8s.ymlee
          kubernetesVersion: "v1.24.1"
          networking:
          podSubnet: 192.168.0.0/16
          serviceSubnet: 20.96.0.0/16
          apiServer:
          certSANs:
          - "${LB_1}"
          controlPlaneEndpoint: "${LB_1}:6443"
          etcd:
          external:
              endpoints:
              - https://${ETCD_1}:2379
              - https://${ETCD_2}:2379
              - https://${ETCD_3}:2379
              caFile: /etc/kubernetes/pki/etcd/ca.crt
              certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
              keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
          ---
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          cgroupDriver: "systemd"
          EOF

          cat kubeadm-config.yaml

          # Check the IP's for the ETCD_1,2,3 and LB_1 are correct, or rebuild it by declaring Parameters again 

5. Create the first control plane nodes with kubeadm init

    5.1 **Final Check**
    - Before kubeadm init, check the followings if the VM is restarted
    - At the master-1 Server
          systemctl restart containerd
          swapoff -a
          sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

          systemctl daemon-reload
          systemctl restart kubelet

          systemctl status kubelet -l
      
    5.2 **Execute kubeadm init**
    - At the master-1 Server

          kubeadm init --config kubeadm-config.yaml --upload-certs

    - After Successful installation, copy the Join tokens key value

          # these are look like ...
          # for control plane

          You can now join any number of the control-plane node running the following command on each as root:

          kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
          --discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06 \
          --control-plane --certificate-key 03796de58479263e9a5554d6e1f520beb48d207d995189d21a3a1478a4f092b5

          Please note that the certificate-key gives access to cluster sensitive data, keep it secret!

          # for control plane

          Then you can join any number of worker nodes by running the following on each as root:
          kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
          --discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06


---

Continue ... 

