1. init 
  -  
    - 
      -  

          export HEKETI_SECRET="4oCcUEFTU1dPUkTigJ0="
          
          cat <<EOF> ~/gluster-secret.yaml
          apiVersion: v1
          kind: Secret
          metadata:
            name: heketi-secret
            namespace: default
          type: "kubernetes.io/glusterfs"
          data:
            key: ${HEKETI_SECRET}
          EOF

          cat ~/gluster-secret.yaml

2. init 
  -  
    - 
      -  

          export HEKETI_API_SERVER=10.178.165.144
          export HEKETI_CLI_CLUSTER=812dc2719135afc4a0b0838f4435008f

          cat <<EOF> ~/glusterfs-sc.yaml
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gluster-heketi
          provisioner: kubernetes.io/glusterfs
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
          allowVolumeExpansion: true
          parameters:
            resturl: http://${HEKETI_API_SERVER}:8080
            restuser: "admin"
            secretName: "heketi-secret"
            secretNamespace: "default"
            volumetype: "replicate:2"
            volumenameprefix: "k8s-dev"
            clusterid: "${HEKETI_CLI_CLUSTER}"
          EOF

          cat ~/glusterfs-sc.yaml

2. init 
  -  
    - 
      -  

          cat <<EOF> ~/gluster-pvc.yaml
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: gluster-pvc
            annotations:
              volume.beta.kubernetes.io/storage-class: gluster-heketi
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 1Gi
          EOF

          cat ~/gluster-pvc.yaml



2. init 
  -  
    - 
      -  
    3.2 Install glusterfs_client.yml
    - with ansible plyabooks    

          mkdir -p ~/ansible-playbooks/glusterfs

          cat <<EOF> ~/ansible-playbooks/glusterfs/install_glusterfs_client.yml
          # install_glusterfs_client.yml
          ---
          - hosts: master:worker
            become: true
            tasks:
              - name: install GlusterFS Client
                yum: 
                  name:
                    - glusterfs-client
          EOF

          cat ~/ansible-playbooks/glusterfs/install_glusterfs_client.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/install_glusterfs_client.yml


2. init 
  -  
    - 
      -  

    3.2 Install glusterfs_server.yml
    - with ansible plyabooks    

          mkdir -p ~/ansible-playbooks/glusterfs

          cat <<EOF> ~/ansible-playbooks/glusterfs/install_glusterfs_server.yml
          # install_glusterfs_server.yml
          ---
          - hosts: glusterfs
            become: true
            tasks:
              - name: edit etc-host for GlusterFS-1 Server
                lineinfile: 
                  path: /etc/hosts
                  line: ${GLUSTERFS_1} glusterfs-1 glusterfs-1.${CLUSTER_DOMAIN}
              - name: edit etc-host for GlusterFS-2 Server
                lineinfile: 
                  path: /etc/hosts
                  line: ${GLUSTERFS_2} glusterfs-2 glusterfs-2.${CLUSTER_DOMAIN}
              - name: edit etc-host for GlusterFS-3 Server
                lineinfile: 
                  path: /etc/hosts
                  line: ${GLUSTERFS_3} glusterfs-3 glusterfs-3.${CLUSTER_DOMAIN}
          - hosts: GLUSTERFS-*
            become: true
            tasks:
              - name: install GlusterFS Server
                yum: 
                  name:
                    - glusterfs-server
          EOF

          cat ~/ansible-playbooks/glusterfs/install_glusterfs_server.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/install_glusterfs_server.yml

2. init 
  -  
    - 
      -  

          cat <<EOF> ~/ansible-playbooks/glusterfs/start-glusterfs.yml
          # start-glusterfs.yml
          ---
          - hosts: GLUSTERFS-*
            become: true
            tasks:
              - name: enable glusterd service
                command: systemctl enable glusterd.service
              - name: start glusterd service
                command: systemctl start glusterd.service
          EOF

          cat ~/ansible-playbooks/glusterfs/start-glusterfs.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/start-glusterfs.yml

    - each glusterfs server

          for i in dm_snapshot dm_mirror dm_thin_pool; do
          > modprobe $i
          > done




2. init 
  -  
    - 
      -  

          cat <<EOF> ~/ansible-playbooks/glusterfs/gluster-peer-probe.yml
          # gluster-peer-probe.yml
          ---
          - hosts: GLUSTERFS-1
            become: true
            tasks:
              - name: Gluster Peer Probe 2
                command: gluster peer probe glusterfs-2
              - name: Gluster Peer Probe 3
                command: gluster peer probe glusterfs-3
          EOF

          cat ~/ansible-playbooks/glusterfs/gluster-peer-probe.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/gluster-peer-probe.yml

                                                                      

    - chekc 

          gluster peer status



2. init 
  -  
    - 
      -  
    3.2 Install heketi-heketi-client.yml
    - with ansible plyabooks    

          mkdir -p ~/ansible-playbooks/glusterfs

          cat <<EOF> ~/ansible-playbooks/glusterfs/install-heketi-heketi-client.yml
          # install-heketi-heketi-client.yml
          ---
          - hosts: HEKETI-0
            become: true
            tasks:
              - name: install Heketi, Heketi Client
                yum: 
                  name:
                    - heketi
                    - heketi-client
          EOF

          cat ~/ansible-playbooks/glusterfs/install-heketi-heketi-client.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/install-heketi-heketi-client.yml


    3.2 edit heketi.json file
    - with ansible plyabooks    

          
          mkdir -p ~/ansible-playbooks/glusterfs

          export HEKETI_PASSWORD=K8sadmin@ds
          
          cat <<EOF> ~/ansible-playbooks/glusterfs/edit_heketi_json.yml
          # edit_heketi_json.yml
          ---
          - hosts: HEKETI-0
            become: true
            tasks:
              - name: set password
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: 'My Secret'
                  replace: "${HEKETI_PASSWORD}"
              - name: set executor to ssh
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: '"executor": "mock"'
                  replace: '"executor": "ssh"'
              - name: set executor to ssh
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: '"keyfile": "path/to/private_key"'
                  replace: '"keyfile": "/etc/heketi/heketi_key"'
              - name: set executor to ssh
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: '"user": "sshuser"'
                  replace: '"user": "root"'
              - name: set executor to ssh
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: "Optional: ssh port.  Default is 22"
                  replace: "22"
              - name: set executor to ssh
                replace:
                  path: /etc/heketi/heketi.json
                  regexp: "Optional: Specify fstab file on node.  Default is /etc/fstab"
                  replace: "/etc/fstab"
          EOF

          cat ~/ansible-playbooks/glusterfs/edit_heketi_json.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/edit_heketi_json.yml

    - at the heketi server    

          ssh-keygen -f /etc/heketi/heketi_key -t rsa -N ''

          chown heketi:heketi /etc/heketi/heketi_key*
          
          ssh-copy-id -i /etc/heketi/heketi_key root@glusterfs-1
          
          ssh-copy-id -i /etc/heketi/heketi_key root@glusterfs-2
          
          ssh-copy-id -i /etc/heketi/heketi_key root@glusterfs-3

          systemctl start heketi
          systemctl status heketi

          curl http://localhost:8080/hello


          export GLUSTERFS_1=10.168.40.32
          export GLUSTERFS_2=10.168.40.51
          export GLUSTERFS_3=10.168.40.10
          export DEVICE_FOR_PVC=/dev/xvdc

          cat <<EOF> /etc/heketi/topology.json
          {
            "clusters": [
              {
                "nodes": [
                  {
                    "node": {
                      "hostnames": {
                        "manage":  [ "glusterfs-1" ],
                        "storage":  [ "${GLUSTERFS_1}" ]
                      },
                      "zone": 1
                    },
                    "devices": [ "${DEVICE_FOR_PVC}"  ]
                  },
                  {
                    "node": {
                      "hostnames": {
                        "manage":  [ "glusterfs-2" ],
                        "storage":  [ "${GLUSTERFS_2}" ]
                      },
                      "zone": 1
                    },
                    "devices": [ "${DEVICE_FOR_PVC}"  ]
                  },
                  {
                    "node": {
                      "hostnames": {
                        "manage":  [ "glusterfs-3" ],
                        "storage":  [ "${GLUSTERFS_3}" ]
                      },
                      "zone": 1
                    },
                    "devices": [ "${DEVICE_FOR_PVC}"  ]
                  }
                ]
              }
            ]
          }
          EOF

          cat /etc/heketi/topology.json



          heketi-cli topology load --user admin --secret hekti_admin_secret --json=/etc/heketi/topology.json


          heketi-cli topology info

          heketi-cli cluster list

          heketi-cli node list


1. init 
  -  
    - 
      -  

          HEKETI_PASSWORD=K8sadmin@ds

          echo -n ${HEKETI_PASSWORD} | base6

          결과값

          export HEKETI_SECRET=결과값
          
          cat <<EOF> ~/gluster-secret.yaml
          apiVersion: v1
          kind: Secret
          metadata:
            name: heketi-secret
            namespace: default
          type: "kubernetes.io/glusterfs"
          data:
            key: ${HEKETI_SECRET}
          EOF

          cat ~/gluster-secret.yaml

          kubectl apply -f ~/gluster-secret.yaml


2. init 
  -  
    - 
      -  

          export HEKETI_0=10.168.40.43

          export HEKETI_API_SERVER=${HEKETI_0}
          export HEKETI_CLI_CLUSTER=c9ee11b5e114c76e69b534f869e9a159 

          cat <<EOF> ~/glusterfs-sc.yaml
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gluster-heketi
          provisioner: kubernetes.io/glusterfs
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
          allowVolumeExpansion: true
          parameters:
            resturl: http://${HEKETI_API_SERVER}:8080
            restuser: "admin"
            secretName: "heketi-secret"
            secretNamespace: "default"
            volumetype: "replicate:3"
            volumenameprefix: "k8s-dev"
            clusterid: "${HEKETI_CLI_CLUSTER}"
          EOF

          cat ~/glusterfs-sc.yaml

          kubectl apply -f ~/glusterfs-sc.yaml


2. default storage class 
  -  
    - 
      -  

          kubectl get sc


          OLD_DEFAULT_SC=gluster-heketi

          kubectl patch storageclass ${OLD_DEFAULT_SC} -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

          NEW_DEFAULT_SC=gluster-heketi

          kubectl patch storageclass ${NEW_DEFAULT_SC} -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

          kubectl get sc


2. init 
  -  
    - 
      -  

          cat <<EOF> ~/gluster-pvc.yaml
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: gluster-pvc
            annotations:
              volume.beta.kubernetes.io/storage-class: gluster-heketi
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 1Gi
          EOF

          cat ~/gluster-pvc.yaml

          kubectl apply -f ~/gluster-pvc.yaml

          kubectl get pv