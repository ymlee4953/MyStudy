# **16-1. Apply CNI Calico (Basic)**

- Air-Gap Environment
- Apply Calico with simple configuration
  - 3 master nodes and 3 worker nodes only
- Calico Version : Latest

--- 

1. Prepare the Calico.yaml

    1.1 Copy the Calico.yaml to master-1 Server
    - To run for the simple configuration

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/calico/copy_calico.yml


    1.2 Apply CNI (calico) yaml   
    - Apply Calico yaml to k8s cluster

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/calico/apply_calico_yaml.yml

    1.3 Check the K8s cluster installation

    - Verify all nodes is ready and all container is running status

          ssh root@${MASTER_1}

          kubectl get nodes -o wide

          kubectl get pods --all-namespaces -o wide

          exit


      - Check the Result : (Sample)

            [root@master-1 ~]# kubectl get nodes -o wide
            NAME                         STATUS   ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
            master-1.1.b.dev.k8s.ymlee   Ready    control-plane,master   29m   v1.20.5   10.178.165.169   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-2.1.b.dev.k8s.ymlee   Ready    control-plane,master   14m   v1.20.5   10.178.165.144   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-3.1.b.dev.k8s.ymlee   Ready    control-plane,master   14m   v1.20.5   10.178.165.160   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-1.1.b.dev.k8s.ymlee   Ready    <none>                 13m   v1.20.5   10.178.165.184   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-2.1.b.dev.k8s.ymlee   Ready    <none>                 13m   v1.20.5   10.178.165.152   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-3.1.b.dev.k8s.ymlee   Ready    <none>                 13m   v1.20.5   10.178.165.175   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl get pods --all-namespaces -o wide
            NAMESPACE     NAME                                                 READY   STATUS    RESTARTS   AGE     IP               NODE                         NOMINATED NODE   READINESS GATES
            kube-system   calico-kube-controllers-7f4f5bf95d-p8ddp             1/1     Running   0          3m39s   192.168.97.1     master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-2d9sh                                    1/1     Running   0          3m39s   10.178.165.184   worker-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-2zp75                                    1/1     Running   0          3m39s   10.178.165.175   worker-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-cfr8r                                    1/1     Running   0          3m39s   10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-dvsfp                                    1/1     Running   0          3m39s   10.178.165.152   worker-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-qsbfn                                    1/1     Running   0          3m39s   10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-t57gw                                    1/1     Running   0          3m39s   10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   coredns-74ff55c5b-g9bwq                              1/1     Running   0          28m     192.168.40.193   worker-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   coredns-74ff55c5b-jrl64                              1/1     Running   0          28m     192.168.167.65   worker-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-1.1.b.dev.k8s.ymlee            1/1     Running   0          28m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-2.1.b.dev.k8s.ymlee            1/1     Running   0          14m     10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-3.1.b.dev.k8s.ymlee            1/1     Running   0          14m     10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-1.1.b.dev.k8s.ymlee   1/1     Running   0          28m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-2.1.b.dev.k8s.ymlee   1/1     Running   0          14m     10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-3.1.b.dev.k8s.ymlee   1/1     Running   0          14m     10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-7z9k8                                     1/1     Running   0          13m     10.178.165.175   worker-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-b5654                                     1/1     Running   0          13m     10.178.165.152   worker-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-c2vqc                                     1/1     Running   0          14m     10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-dnvl9                                     1/1     Running   0          13m     10.178.165.184   worker-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-pvhkm                                     1/1     Running   0          14m     10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-qmtjb                                     1/1     Running   0          28m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-1.1.b.dev.k8s.ymlee            1/1     Running   0          28m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-2.1.b.dev.k8s.ymlee            1/1     Running   0          14m     10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-3.1.b.dev.k8s.ymlee            1/1     Running   0          14m     10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>


---
# **Done : One K8s cluster was just created !**