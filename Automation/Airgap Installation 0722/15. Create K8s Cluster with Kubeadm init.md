# **15. Create K8s Cluster with Kubeadm init**

- Air-Gap Environment
- Run kubeadm init command in the master-1 server
- get the join 
- make parameter for master node join and worker node join
- Run the rest join commnad with anisible playbooks


1. kubeadm init to make a kubernetes cluster 
  
    1.1 Run the kubeadm init command at the master-1 node
    
    - move to the master-1 node
      
          ssh ${MASTER_1}
    
    - Runte the kubeadm init command 

          kubeadm init --config kubeadm-config.yaml --upload-certs

      - Check the Result : (Sample)

            Your Kubernetes control-plane has initialized successfully!

            To start using your cluster, you need to run the following as a regular user:

            mkdir -p $HOME/.kube
            sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config

            Alternatively, if you are the root user, you can run:

            export KUBECONFIG=/etc/kubernetes/admin.conf

            You should now deploy a pod network to the cluster.
            Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
            https://kubernetes.io/docs/concepts/cluster-administration/addons/

            You can now join any number of the control-plane node running the following command on each as root:

            kubeadm join 10.178.165.138:6443 --token 0nztf6.5nqq0z2tmaexzazw \
            --discovery-token-ca-cert-hash sha256:782110d6e97210822dbb5d56e1a4174883accfdd9dc90f434fbddc7242557a27 \
            --control-plane --certificate-key f21208684cfef625d208130e0f57c9ff18cf332a1437e063c328b02124c01f11

            Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
            As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
            "kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

            Then you can join any number of worker nodes by running the following on each as root:

            kubeadm join 10.178.165.138:6443 --token 0nztf6.5nqq0z2tmaexzazw \
            --discovery-token-ca-cert-hash sha256:782110d6e97210822dbb5d56e1a4174883accfdd9dc90f434fbddc7242557a27


    - Baak to the ansible Server

          exit

    1.2 Prepare k8s cluster join paremeters and cert files 

    - Make the parameters to join with the above result

          export KUBEADM_JOIN_MASTER="kubeadm join xxx.xxx.xxx.xxx:6443 --token xxxxxxxxxxxxxxxxxxxxxx --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --control-plane --certificate-key xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
          
          export KUBEADM_JOIN_WORKER="kubeadm join xxx.xxx.xxx.xxx:6443 --token xxxxxxxxxxxxxxxxxxxxxx --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"


    - copy certs to master2,3

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/copy_kubernetes_certs.yml

    1.3 Create ansible playbooks to execute K8s Cluster join commands and Run

    - For master nodes 

          cat <<EOF> ~/ansible-playbooks/kubernetes/kubeadm_join_master.yml
          # kubeadm_join_master.yml
          ---
          - hosts: MASTER-2, MASTER-3
            become: true
            tasks:
              - name: kubeadm_join_master
                command: ${KUBEADM_JOIN_MASTER}           
          EOF

          cat ~/ansible-playbooks/kubernetes/kubeadm_join_master.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/kubeadm_join_master.yml

    - For worker nodes (including router, infra nodes)

          cat <<EOF> ~/ansible-playbooks/kubernetes/kubeadm_join_worker.yml
          # kubeadm_join_worker.yml
          ---
          - hosts: worker:router:infra
            become: true
            tasks:
              - name: kubeadm_join_worker
                command: ${KUBEADM_JOIN_WORKER}           
          EOF

          cat ~/ansible-playbooks/kubernetes/kubeadm_join_worker.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/kubeadm_join_worker.yml

    - Enable kubectl for each master nodes
 
          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/enable_kubectl_to_master.yml

    - Verify the k8s cluster installation 

          ssh root@${MASTER_1}

          kubectl get nodes -o wide

          kubectl get pods --all-namespaces -o wide

          exit

      - Check the Result : (Sample)
      - Check the Status is not Ready for all cluster nodes (until CRI installed)
      - Check the coredns are not Ready and in the Pending status (until CRI installed)

            [root@master-1 ~]# kubectl get nodes -o wide
            NAME                         STATUS     ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
            master-1.1.b.dev.k8s.ymlee   NotReady   control-plane,master   16m     v1.20.5   10.178.165.169   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-2.1.b.dev.k8s.ymlee   NotReady   control-plane,master   2m29s   v1.20.5   10.178.165.144   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-3.1.b.dev.k8s.ymlee   NotReady   control-plane,master   2m29s   v1.20.5   10.178.165.160   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-1.1.b.dev.k8s.ymlee   NotReady   <none>                 101s    v1.20.5   10.178.165.184   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-2.1.b.dev.k8s.ymlee   NotReady   <none>                 101s    v1.20.5   10.178.165.152   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-3.1.b.dev.k8s.ymlee   NotReady   <none>                 101s    v1.20.5   10.178.165.175   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl get pods --all-namespaces -o wide
            NAMESPACE     NAME                                                 READY   STATUS    RESTARTS   AGE     IP               NODE                         NOMINATED NODE   READINESS GATES
            kube-system   coredns-74ff55c5b-g9bwq                              0/1     Pending   0          16m     <none>           <none>                       <none>           <none>
            kube-system   coredns-74ff55c5b-jrl64                              0/1     Pending   0          16m     <none>           <none>                       <none>           <none>
            kube-system   kube-apiserver-master-1.1.b.dev.k8s.ymlee            1/1     Running   0          16m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-2.1.b.dev.k8s.ymlee            1/1     Running   0          2m28s   10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-3.1.b.dev.k8s.ymlee            1/1     Running   0          2m29s   10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-1.1.b.dev.k8s.ymlee   1/1     Running   0          16m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-2.1.b.dev.k8s.ymlee   1/1     Running   0          2m28s   10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-3.1.b.dev.k8s.ymlee   1/1     Running   0          2m28s   10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-7z9k8                                     1/1     Running   0          102s    10.178.165.175   worker-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-b5654                                     1/1     Running   0          102s    10.178.165.152   worker-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-c2vqc                                     1/1     Running   0          2m30s   10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-dnvl9                                     1/1     Running   0          102s    10.178.165.184   worker-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-pvhkm                                     1/1     Running   0          2m30s   10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-qmtjb                                     1/1     Running   0          16m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-1.1.b.dev.k8s.ymlee            1/1     Running   0          16m     10.178.165.169   master-1.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-2.1.b.dev.k8s.ymlee            1/1     Running   0          2m28s   10.178.165.144   master-2.1.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-3.1.b.dev.k8s.ymlee            1/1     Running   0          2m29s   10.178.165.160   master-3.1.b.dev.k8s.ymlee   <none>           <none>
