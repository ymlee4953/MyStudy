# **16-2. Apply CNI Calico (Typha)**

- Air-Gap Environment
- Apply Calico with Typha configuration
  - Typha supports large number of worker nodes (50+)
  - 3 master nodes, 3 worker nodes, 3 router nodes, (2 infra nodes)
- To make router node to be the typha node, set label first
- Calico Version : Latest

--- 

1. Set node label for typha node

    1.1 Cooy label script to master-1   
    - copy the file and run

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/copy_kubectl_label_sh.yml


2. Prepare the Calico-typha.yaml

    2.1 Edit the Calico.yaml
    - Edit the calico-typha yaml to assign the typha to the router nodes

          vi ~/files/BASTION-0/calico/calico-typha.yaml
          
      - Insert value "node-type: router" into Deployment "calico-typha.spec.template.spec.nodeSelect"

            ...

            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: calico-typha
              namespace: kube-system
              labels:
                k8s-app: calico-typha
            spec:
            ...
              template:
                ...
                spec:
                  nodeSelector:
                    kubernetes.io/os: linux
                    node-type: router
                  ...


    2.2 Copy the Calico-typha.yaml to master-1 Server
    - to the master-1

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/calico/copy_calico_typha.yml


    2.3 apply calico_typha yaml   
    - apply

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/calico/apply_calico_typha_yaml.yml


    2.4 Check the K8s cluster installation

    - Verify all nodes is ready and all container is running status

          ssh root@${MASTER_1}

          kubectl get nodes -o wide

          kubectl get pods --all-namespaces -o wide

          exit


      - Check the Result : (Sample)

            [root@master-1 ~]# kubectl get nodes -o wide
            NAME                         STATUS   ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
            master-1.2.b.dev.k8s.ymlee   Ready    control-plane,master   27m   v1.20.5   10.178.165.154   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-2.2.b.dev.k8s.ymlee   Ready    control-plane,master   24m   v1.20.5   10.178.165.130   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            master-3.2.b.dev.k8s.ymlee   Ready    control-plane,master   24m   v1.20.5   10.178.165.132   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            router-1.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.148   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            router-2.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.179   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            router-3.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.185   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-1.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.188   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-2.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.161   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            worker-3.2.b.dev.k8s.ymlee   Ready    <none>                 23m   v1.20.5   10.178.165.174   <none>        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   containerd://1.4.8
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl get pods --all-namespaces -o wide
            NAMESPACE     NAME                                                 READY   STATUS    RESTARTS   AGE     IP                NODE                         NOMINATED NODE   READINESS GATES
            kube-system   calico-kube-controllers-7f4f5bf95d-g9k4g             1/1     Running   0          7m12s   192.168.221.129   router-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-4bllh                                    1/1     Running   0          7m12s   10.178.165.130    master-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-77qll                                    1/1     Running   0          7m12s   10.178.165.185    router-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-97l4m                                    1/1     Running   0          7m12s   10.178.165.188    worker-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-hbr5z                                    1/1     Running   0          7m12s   10.178.165.179    router-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-jq6jj                                    1/1     Running   0          7m12s   10.178.165.132    master-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-m8c9g                                    1/1     Running   0          7m12s   10.178.165.174    worker-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-phrzt                                    1/1     Running   0          7m12s   10.178.165.148    router-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-wm98w                                    1/1     Running   0          7m12s   10.178.165.154    master-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-node-xlzx5                                    1/1     Running   0          7m12s   10.178.165.161    worker-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   calico-typha-75949b9c8-lbbbl                         1/1     Running   0          7m12s   10.178.165.148    router-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   coredns-74ff55c5b-59gvp                              1/1     Running   0          26m     192.168.42.193    worker-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   coredns-74ff55c5b-crrbp                              1/1     Running   0          26m     192.168.131.193   worker-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-1.2.b.dev.k8s.ymlee            1/1     Running   0          26m     10.178.165.154    master-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-2.2.b.dev.k8s.ymlee            1/1     Running   0          24m     10.178.165.130    master-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-apiserver-master-3.2.b.dev.k8s.ymlee            1/1     Running   0          24m     10.178.165.132    master-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-1.2.b.dev.k8s.ymlee   1/1     Running   0          27m     10.178.165.154    master-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-2.2.b.dev.k8s.ymlee   1/1     Running   0          24m     10.178.165.130    master-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-controller-manager-master-3.2.b.dev.k8s.ymlee   1/1     Running   0          24m     10.178.165.132    master-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-6cmxg                                     1/1     Running   0          23m     10.178.165.179    router-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-857kv                                     1/1     Running   0          23m     10.178.165.174    worker-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-f6m25                                     1/1     Running   0          23m     10.178.165.185    router-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-mr97j                                     1/1     Running   0          26m     10.178.165.154    master-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-pc7jg                                     1/1     Running   0          24m     10.178.165.132    master-3.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-r5dlb                                     1/1     Running   0          23m     10.178.165.161    worker-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-v255n                                     1/1     Running   0          23m     10.178.165.188    worker-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-x6skr                                     1/1     Running   0          24m     10.178.165.130    master-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-proxy-zdgn6                                     1/1     Running   0          23m     10.178.165.148    router-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-1.2.b.dev.k8s.ymlee            1/1     Running   0          27m     10.178.165.154    master-1.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-2.2.b.dev.k8s.ymlee            1/1     Running   0          24m     10.178.165.130    master-2.2.b.dev.k8s.ymlee   <none>           <none>
            kube-system   kube-scheduler-master-3.2.b.dev.k8s.ymlee            1/1     Running   0          24m     10.178.165.132    master-3.2.b.dev.k8s.ymlee   <none>           <none>
            [root@master-1 ~]#
            