# **01. Setup Bastion Server**

## **Ansible**

1. Install Ansible 
          
    1.1. Install stable and latest version
          
    - d
          
          sudo yum install -y epel-release
          
          yum repolist

          sudo yum install -y ansible
          
          ansible --version
          
          ssh-keygen

    - Task 1122
    - Check the SSH_key 

          cat /root/.ssh/id_rsa.pub
          
          yum install -y wget

          yum install -y bind-utils
---
## **For External etcd**

4. Download Software Package for etcdadm
    

    - Task 
    4.1 install Git and make
    - yum install and make

          yum install -y make git

          wget https://dl.google.com/go/go1.16.2.linux-amd64.tar.gz

          tar -C /usr/local -xzf ./go1.16.2.linux-amd64.tar.gz

          vi ~/.bash_profile

    - edit Go Path
    
          ...
          PATH=$PATH:$HOME/bin:/usr/local/go/bin
          ...
    
    - apply profile 

          source ~/.bash_profile

          git clone https://github.com/kubernetes-sigs/etcdadm.git

          cd etcdadm

          make etcdadm 

          cat constants/constants.go | grep DefaultVersion

    - the result : the default etcd version of etcdadm is "3.5.0"

          ==> DefaultVersion = "3.5.0"

    - Download etcd installation file

          wget https://github.com/coreos/etcd/releases/download/v3.5.0/etcd-v3.5.0-linux-amd64.tar.gz
   
          mkdir -p ~/files/BASTION-0/etcdadm/

          cp ~/etcdadm/etcdadm ~/files/BASTION-0/etcdadm/

          cp ~/etcdadm/etcd-v3.5.0-linux-amd64.tar.gz ~/files/BASTION-0/etcdadm/

          cd 
          wget https://www.haproxy.org/download/1.7/src/haproxy-1.7.8.tar.gz

          mkdir -p ~/files/BASTION-0/haproxy/
          mv ./haproxy-1.7.8.tar.gz ~/files/BASTION-0/haproxy/

          mkdir -p ~/files/BASTION-0/calico

          curl https://docs.projectcalico.org/manifests/calico.yaml -o calico.yaml
          curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico-typha.yaml

          mv ./calico*.yaml ~/files/BASTION-0/calico

          mkdir -p ~/files/BASTION-0/kubesphere/v3.1.0

          cd ~/files/BASTION-0/kubesphere/v3.1.0

          curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.1.0/cluster-configuration.yaml
          curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.1.0/kubesphere-installer.yaml

          mkdir -p ~/files/BASTION-0/kubesphere/v3.1.1

          cd ~/files/BASTION-0/kubesphere/v3.1.1

          curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/cluster-configuration.yaml
          curl -L -O https://github.com/kubesphere/ks-installer/releases/download/v3.1.1/kubesphere-installer.yaml

          cp ./cluster-configuration.yaml ../  
          cp ./kubesphere-installer.yaml ../

          cd ~/


# **04. Setup Nexus Server**

## **Nexus Server Preparation**

- Target Server

    - NEXUS_0

---

       
1. Base Package Installation
    - **At the Nexus server**
  
          yum update -y

          yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel

          yum install -y wget

          cd /opt
          wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz
          tar -xvzf latest-unix.tar.gz

          mv nexus-3.* nexus
          mv sonatype-work nexusdata
          
          useradd --system nexus
          
          chown -R nexus:nexus /opt/nexus
          chown -R nexus:nexus /opt/nexusdata
  
          mv /opt/nexus/bin/nexus.vmoptions /opt/nexus/bin/nexus.vmoptions.bak

          sed 's/sonatype-work/nexusdata/g' /opt/nexus/bin/nexus.vmoptions.bak > /opt/nexus/bin/nexus.vmoptions

          cat /opt/nexus/bin/nexus.vmoptions 

          mv /opt/nexus/bin/nexus.rc /opt/nexus/bin/nexus.rc.bak
          
          sed 's/#run_as_user=""/run_as_user="nexus"/g' /opt/nexus/bin/nexus.rc.bak > /opt/nexus/bin/nexus.rc

          cat /opt/nexus/bin/nexus.rc

          cp /etc/security/limits.conf /etc/security/limits.conf.bak
          
          cat <<EOF >> /etc/security/limits.conf
          nexus - nofile 65536
          EOF

          cat /etc/security/limits.conf

          cat <<EOF > /etc/systemd/system/nexus.service 
          [Unit]
          Description=Nexus Service
          After=syslog.target network.target

          [Service]
          Type=forking
          LimitNOFILE=65536
          ExecStart=/opt/nexus/bin/nexus start
          ExecStop=/opt/nexus/bin/nexus stop
          User=nexus
          Group=nexus
          Restart=on-failure
          
          [Install]
          WantedBy=multi-user.target
          EOF

          cat /etc/systemd/system/nexus.service 

          systemctl daemon-reload
          systemctl enable nexus.service
          systemctl start nexus.service

          tail -f /opt/nexusdata/nexus3/log/nexus.log

    - Result

          # then
          # if nexus service is successfully started, you can see the message like this, ... 
          ...

          -------------------------------------------------
          
          Started Sonatype Nexus OSS 3.xx.x-xx
          
          -------------------------------------------------

6. Initial Connection via browser

    6.1 get password
    - **at the nexus server**

          cat /opt/nexusdata/nexus3/admin.password

    - then the initial password will show-up like 
 
          275bdee3-0c71-4b60-9947-eea20d946e1f
            
          # copy the initial password
          # you need to change the password at the first login 

    6.2 First log in via browser
    - **at the web browser**
    - Connect with port :8081
      
          Public : http://***.***.***.***:8081/
          Private : http://***.***.***.***:8081/

    6.3 Change the password in the First login
    - **at the browser**

          # user: admin
          # Password : paste copied the initial password and change to new password through Wizard


7. Repository Set up

    7.1 Yum Repository

    - yum (proxy)

      | Source | Repository Name | Remote Storage (URL) |
      | :--- | :--- | :--- |
      | CentOS – Base | yumpxy-centos-base | http://mirror.centos.org/centos/7/os/x86_64/ |
      | CentOS – Extra | yumpxy-centos-extra | http://mirror.centos.org/centos/7/extras/x86_64/ |
      | Docker-CE | yumpxy-docker-ce | https://download.docker.com/linux/centos/7/x86_64/stable |
      | K8s.gcr.io | yumpxy-k8s-gcr-io | https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 |
      | | yumpxy-centos-storage | http://mirror.kakao.com/centos/7.9.2009/storage/x86_64/gluster-9/ |

    - yum (hosted)
      | Repository Name | Repository Depth |
      | :--- | :--- |
      | yum-hosted | 1 |
      
    - yum (group)
      | Repository Name | Member Repositories |
      |  :--- | :--- |
      | yum-group | yumpxy-centos-base |
      | | yumpxy-centos-extra |
      | | yumpxy-docker-ce |
      | | yum-hosted |
      | | yumpxy-k8s-gcr-io |
      | | yumpxy-centos-storage | 


    7.2 Docker Repository
    - docker (proxy)  
      | Source | Repository Name | Remote Storage (URL) | Extra Oprtions|
      | :--- | :--- | :--- | :--- |
      | Docker.io | dockerpxy-docker-io | https://registry-1.docker.io/ | Allow anonymous docker pull |
      | Gcr.io | dockerpxy-gcr-io | https://gcr.io/ | Allow anonymous docker pull |
      | Github.io | dockerpxy-ghcr-io | http://ghcr.io/ | Allow anonymous docker pull |
      | K8s.gcr.io | dockerpxy-k8s-gcr-io | https://k8s.gcr.io/ | Allow anonymous docker pull |
      | Quay.io | dockerpxy-quay-io | http://quay.io/ | Allow anonymous docker pull |
      | quay.io(Azure Ch) | dockerpxy-quay-azc-io | https://quay.azk8s.cn/ | Allow anonymous docker pull |
      | gcr.io (Azure Ch) | dockerpxy-gcr-azc-io | https://gcr.azk8s.cn/ | Allow anonymous docker pull |
      | elastic.co | docker-elastic-co | http://docker.elastic.co/ | Allow anonymous docker pull |


    - docker (hosted)
      | Repository Name | HTTP | Extra Oprtions|
      | :--- | :--- | :--- |
      | docker-hosted | 5000 | Allow anonymous docker pull |
      
      - 무료 Nexus 버전에서는 Docker Group 으로 Push 가 안되어 docker (hosted)로 직접 해야함
      
    - docker (group)
      | Repository Name | HTTP | Member Repositories | Extra Oprtions|
      |  :--- | :--- | :--- | :--- |
      | docker-group | 5001 | docker-hosted | Allow anonymous docker pull |
      | | | dockerpxy-docker-io |
      | | | dockerpxy-gcr-io |
      | | | dockerpxy-ghcr-io |
      | | | dockerpxy-k8s-gcr-io |
      | | | dockerpxy-quay-io|
      | | | docker-elastic-co |
      | | | dockerpxy-gcr-azc-io |
      | | | dockerpxy-quay-azc-io|

    7.3 Pypi Repository
    - pypi (proxy)  

      | Source | Repository Name | Remote Storage (URL) |
      | :--- | :--- | :--- |
      | Python Package Index | pypi-proxy | https://pypi.org | 

    7.4 add Realms
    - Administration > Security > Realms       
      
          # Select Realms Menu
         
          # Move "Docker Bearer Token Realm" from Available to Active

8. Restart Nexus Service

    8.1 Restart Sevice to apply the change
    - **at the Nexus Server**

          systemctl restart nexus.service

--- 
# **Done**                          


# **02. Prepare Default Configurations**

## Initialize

1. Create Directoy for new Cluster

    1.1. Create Directory

    - At the bastion/ansible Server

          mkdir -p $HOME/ansible-playbooks
          mkdir -p $HOME/configurations
          mkdir -p $HOME/files

          yum install -y python-passlib

          mkdir -p $HOME/ansible-playbooks/initialize

          cat <<EOF> $HOME/ansible-playbooks/initialize/change-password.yml
          # change-password.yml
          ---
          - hosts: etcd:master:worker:lb*:router:infra:shared:glusterfs
            become: yes
            tasks:
              - name: Change user password
                user:
                  name: root
                  update_password: always
                  password: "{{ newpassword|password_hash('sha512') }}"
          EOF

          cat $HOME/ansible-playbooks/initialize/change-password.yml

          cat <<EOF> $HOME/ansible-playbooks/initialize/set_yum_repo_glusterfs.yml
          # set_yum_repo_glusterfs.yml
          ---
          - hosts: glusterfs
            become: true
            tasks:
              - name: copy airgap YUM Repo File
                copy:
                  src: $HOME/configurations/etc/yum.repos.d/nexus.repo
                  dest: /etc/yum.repos.d/nexus.repo
          EOF

          cat $HOME/ansible-playbooks/initialize/set_yum_repo_glusterfs.yml

          cat <<EOF> $HOME/ansible-playbooks/initialize/set_yum_repo.yml
          # set_yum_repo.yml
          ---
          - hosts: master:worker:lb*:router:infra
            become: true
            tasks:
              - name: copy airgap YUM Repo File
                copy:
                  src: $HOME/configurations/etc/yum.repos.d/nexus.repo
                  dest: /etc/yum.repos.d/nexus.repo
          EOF

          cat $HOME/ansible-playbooks/initialize/set_yum_repo.yml

---
## External ETCD

4. Copy etcd files from Bastion to etcd Servers

    4.1. Copy etcdadm and etcd installation files from Bastion
    
    - create ansible playbook

          mkdir -p ~/ansible-playbooks/etcd/

          cat <<EOF> ~/ansible-playbooks/etcd/copy_etcdadm.yml
          # copy_etcdadm.yml
          ---
          - hosts: etcd
            become: true
            tasks:           
              - name: Copy the file to etcd
                copy:
                  src:  ~/files/BASTION-0/etcdadm/etcdadm
                  dest: ~/
          - hosts: etcd
            become: true
            tasks:           
              - name: chmod of etcdadm
                ansible.builtin.file:
                  path: ~/etcdadm
                  mode: '0755'
          - hosts: etcd
            become: true
            tasks:           
              - name: Copy the tar file to etcd v3.5.0
                copy:
                  src:  ~/files/BASTION-0/etcdadm/etcd-v3.5.0-linux-amd64.tar.gz
                  dest: /var/cache/etcdadm/etcd/v3.5.0/
          EOF

          cat ~/ansible-playbooks/etcd/copy_etcdadm.yml


   
          cat <<EOF> ~/ansible-playbooks/etcd/etcdadm_init.yml
          # etcdadm_init.yml
          ---
          - hosts: ETCD-1
            become: true
            tasks:
              - name: initialize etcd cluster
                command: ~/etcdadm init
          EOF

          cat ~/ansible-playbooks/etcd/etcdadm_init.yml


          mkdir -p ~/files

          cat <<EOF> ~/ansible-playbooks/etcd/copy_etcd_cert.yml
          # copy_etcd_cert_.yml
          ---
          - hosts: ETCD-1
            become: true
            tasks:
              - name: Fetch the etcd-cert files from the ETCD_1
                fetch: 
                  src: /etc/etcd/pki/{{ item }}
                  dest: ~/files/
                with_items:
                - ca.crt
                - ca.key  
          - hosts: ETCD-2:ETCD-3
            become: true
            tasks:           
              - name: Copy the file to etcd2 etcd3
                copy:
                  src:  ~/files/ETCD-1/etc/etcd/pki/{{ item }}
                  dest: /etc/etcd/pki/
                with_items:
                - ca.crt
                - ca.key  
          EOF

          cat ~/ansible-playbooks/etcd/copy_etcd_cert.yml


          mkdir -p ~/ansible-playbooks/LB

          cat <<EOF> ~/ansible-playbooks/LB/base_package_for_haproxy.yml
          # base_package_for_haproxy.yml
          ---
          - hosts: lb*
            become: true
            tasks:
              - name: install base package 
                yum:
                  name: 
                    - gcc
                    - pcre-static
                    - pcre-devel
                    - wget
                    - make
                    - openssl-devel
          EOF
          cat ~/ansible-playbooks/LB/base_package_for_haproxy.yml


          cat <<EOF> ~/ansible-playbooks/LB/install_haproxy.yml
          # install_haproxy.yml
          - hosts: lb*
            become: true
            tasks: 
              - name: Copy the file to the haproxy
                copy: 
                  src: ~/files/BASTION-0/haproxy/haproxy-1.7.8.tar.gz
                  dest: /opt/
              - name: unarchive haproxy-1.7.8.tar.gz
                unarchive:
                  src: ~/files/BASTION-0/haproxy/haproxy-1.7.8.tar.gz
                  dest: /opt/
              - name: make directory at loadbalancer servers
                file:                  
                  path: "{{ item }}"
                  state: directory
                loop:            
                  - /opt/haproxy-1.7.8/
                  - /etc/haproxy/
                  - /var/lib/haproxy/
              - name: make haproxy files
                make:
                  chdir: /opt/haproxy-1.7.8
                  params:
                    TARGET=linux2628
                    USE_OPENSSL=1
              - name: run haproxy install files
                make:
                  chdir: /opt/haproxy-1.7.8
                  target: install
              - name: touch files
                file:
                  path: /var/lib/haproxy/stats
                  state: touch
              - name: Create symbolic link 
                file:
                  src: "/usr/local/sbin/haproxy"
                  dest: "/usr/sbin/haproxy"
                  state: link
              - name: cp haproxy.init   
                command: 
                  cp /opt/haproxy-1.7.8/examples/haproxy.init /etc/init.d/haproxy
              - name: chmod haproxy.init 755  
                file: 
                  path: /etc/init.d/haproxy
                  mode: 0755          
              - name: create system user
                ansible.builtin.user:
                  name: haproxy
                  system: yes
                  state: present                  
          EOF

          cat ~/ansible-playbooks/LB/install_haproxy.yml



          mkdir -p ~/configurations/LB/etc/haproxy

          cat <<EOF > ~/configurations/LB/etc/haproxy/haproxy.cfg 
          global
                log /dev/log local0
                log /dev/log local1 notice
                chroot /var/lib/haproxy
                stats timeout 30s
                user haproxy
                group haproxy
                daemon
            
          defaults
                maxconn 20000
                mode    tcp
                option  dontlognull
                timeout http-request 10s
                timeout queue        1m
                timeout connect      10s
                timeout client       86400s
                timeout server       86400s
                timeout tunnel       86400s
          EOF

          cat ~/configurations/LB/etc/haproxy/haproxy.cfg 

          mkdir -p ~/configurations/LB-1/etc/haproxy
          mkdir -p ~/configurations/LB-2/etc/haproxy          


          cat <<EOF> ~/ansible-playbooks/LB/start_haproxy_1.yml
          # start_haproxy_1.yml
          ---
          - hosts: LB-1
            become: true
            tasks:
              - name: copy haproxy config file
                copy:
                  src: ~/configurations/LB-1/etc/haproxy/haproxy.cfg
                  dest: /etc/haproxy/haproxy.cfg
              - name: daemon-reload
                command: systemctl daemon-reload
              - name: start haproxy
                command: systemctl start haproxy
              - name: enable haproxy
                command: systemctl enable haproxy
              - name: status haproxy
                command: systemctl status haproxy
          EOF

          cat ~/ansible-playbooks/LB/start_haproxy_1.yml

          cat <<EOF> ~/ansible-playbooks/LB/start_haproxy_2.yml
          # start_haproxy_2.yml
          ---
          - hosts: LB-2
            become: true
            tasks:
              - name: copy haproxy config file
                copy:
                  src: ~/configurations/LB-2/etc/haproxy/haproxy.cfg
                  dest: /etc/haproxy/haproxy.cfg
              - name: daemon-reload
                command: systemctl daemon-reload
              - name: start haproxy
                command: systemctl start haproxy
              - name: enable haproxy
                command: systemctl enable haproxy
              - name: status haproxy
                command: systemctl status haproxy
          EOF

          cat ~/ansible-playbooks/LB/start_haproxy_2.yml
---
## VM setup

6. VM setup

    6.1. Install Linux Netcat to each K8s master server

    - at ansible Server

          cat <<EOF> ~/ansible-playbooks/initialize/yum_install_nc.yml
          # yum_install_nc.yml
          ---
          - hosts: master
            become: true
            tasks:
              - name: yum install nc
                yum:
                  name:
                    - nc
          EOF

          cat ~/ansible-playbooks/initialize/yum_install_nc.yml

          mkdir -p ~/configurations/etc/modules-load.d/

          cat <<EOF | tee ~/configurations/etc/modules-load.d/containerd.conf
          overlay
          br_netfilter
          EOF

          cat ~/configurations/etc/modules-load.d/containerd.conf

          cat <<EOF | tee ~/configurations/etc/modules-load.d/k8s.conf
          br_netfilter
          EOF

          cat ~/configurations/etc/modules-load.d/k8s.conf

          mkdir -p ~/configurations/etc/sysctl.d/

          cat <<EOF | tee ~/configurations/etc/sysctl.d/99-kubernetes-cri.conf
          net.bridge.bridge-nf-call-iptables  = 1
          net.ipv4.ip_forward                 = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          EOF

          cat ~/configurations/etc/sysctl.d/99-kubernetes-cri.conf

          cat <<EOF | tee ~/configurations/etc/sysctl.d/k8s.conf
          net.bridge.bridge-nf-call-ip6tables = 1
          net.bridge.bridge-nf-call-iptables = 1
          EOF

          cat ~/configurations/etc/sysctl.d/k8s.conf


          mkdir -p ~/ansible-playbooks/containerd/

          cat <<EOF> ~/ansible-playbooks/containerd/base_for_containerd.yml          
          # base_for_containerd.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: copy configurations files containerd.conf
                copy:
                  src: ~/configurations/etc/modules-load.d/containerd.conf
                  dest: /etc/modules-load.d/
              - name: modprobe overlay
                command: modprobe overlay
              - name: modprobe br_netfilter
                command: modprobe br_netfilter
              - name: copy configurations files containerd.conf
                copy:
                  src: ~/configurations/etc/sysctl.d/99-kubernetes-cri.conf
                  dest: /etc/sysctl.d/
              - name: sysctl system
                command: sysctl --system
          EOF

          cat ~/ansible-playbooks/containerd/base_for_containerd.yml
          

          cat <<EOF> ~/ansible-playbooks/containerd/install_containerd.yml
          # install_containerd-k8s_base.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: install containerd
                yum: 
                  name:
                    - containerd.io
          EOF

          cat ~/ansible-playbooks/containerd/install_containerd.yml

         
          cat <<EOF> ~/ansible-playbooks/containerd/start_containerd.yml
          # start_containerd.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            
            tasks:
              - name: start containerd
                command: 
                  systemctl restart containerd
              - name: enable containerd
                command: 
                  systemctl enable containerd
          EOF

          cat ~/ansible-playbooks/containerd/start_containerd.yml


          mkdir -p ~/ansible-playbooks/kubernetes/

          cat <<EOF> ~/ansible-playbooks/kubernetes/base_for_kubernetes.yml
          # base_for_kubernetes.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: copy configurations files k8s.conf to module-load.d
                copy:
                  src: ~/configurations/etc/modules-load.d/k8s.conf
                  dest: /etc/modules-load.d/
              - name: copy configurations files k8s.conf to sysctl.d
                copy:
                  src: ~/configurations/etc/sysctl.d/k8s.conf
                  dest: /etc/sysctl.d/
              - name: sysctl system
                command: sysctl --system
              - name: setenforce 0
                command: setenforce 0
              - name: sed /etc/selinux/config
                command: sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
              - name: swapoff 
                command: swapoff -a
              - name: sed /etc/fstab
                command: sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
          EOF

          cat ~/ansible-playbooks/kubernetes/base_for_kubernetes.yml


          cat <<EOF> ~/ansible-playbooks/containerd/install_kubernetes.yml
          # install_kubernetes.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: install kubernetes
                yum: 
                  name:
                    - kubelet-1.20.5
                    - kubeadm-1.20.5
                    - kubectl-1.20.5
              - name: enable kubelet service
                command: systemctl enable kubelet.service
              - name: download 
                command: kubeadm config images pull --kubernetes-version v1.20.5
          EOF

          cat ~/ansible-playbooks/containerd/install_kubernetes.yml

          mkdir -p ~/ansible-playbooks/kubernetes/

          cat <<EOF> ~/ansible-playbooks/kubernetes/modify_systemd_configuration.yml
          # modify_systemd_configuration.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: insertafter
                lineinfile:
                  path: /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
                  insertafter: 'Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"'
                  line: 'Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"'
              - name: Add CGROUP ARGS line
                lineinfile:
                  path: /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
                  backrefs: true
                  regexp: "^(.*/usr/bin/kubelet.*)$"
                  line: '\1 \$KUBELET_CGROUP_ARGS' 
          EOF
          
          cat ~/ansible-playbooks/kubernetes/modify_systemd_configuration.yml

    
          cat <<EOF> ~/ansible-playbooks/kubernetes/restart_kubelet.yml
          # restart_kubelet.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: daemon-reload
                command: systemctl daemon-reload
              - name: start kubelet
                command: systemctl start kubelet
              - name: enable kubelet
                command: systemctl enable kubelet            
          EOF

          cat ~/ansible-playbooks/kubernetes/restart_kubelet.yml

---
## Kubeadm

7. Copy Etcd cert to master-1

    7.1. Fetch etcd cert to Bastion
    - with ansible playbook

          cat <<EOF> ~/ansible-playbooks/kubernetes/fetch_etcd_cert_file.yml
          # fetch_etcd_cert_file.yml
          - hosts: ETCD-1
            become: true
            tasks:
              - name: Fetch the etcd cert files from the ETCD_1
                fetch: 
                  src: /etc/etcd/pki/{{ item }}
                  dest: ~/files/
                with_items:
                - apiserver-etcd-client.crt
                - apiserver-etcd-client.key
          EOF
          
          cat ~/ansible-playbooks/kubernetes/fetch_etcd_cert_file.yml

          cat <<EOF> ~/ansible-playbooks/kubernetes/copy_etcd_cert_file.yml
          # copy_etcd_cert_file.yml
          - hosts: MASTER-1
            become: true
            tasks: 
              - name: make a directory for etcd certs
                file:
                  path: /etc/kubernetes/pki/etcd/
                  state: directory
              - name: Copy the ca file to the etcd directory
                copy: 
                  src: ~/files/ETCD-1/etc/etcd/pki/{{ item }}
                  dest: /etc/kubernetes/pki/etcd
                with_items:
                - ca.crt
                - ca.key
              - name: Copy the file to the pki directory
                copy: 
                  src: ~/files/ETCD-1/etc/etcd/pki/{{ item }}
                  dest: /etc/kubernetes/pki
                with_items:
                - apiserver-etcd-client.crt
                - apiserver-etcd-client.key
          EOF

          cat ~/ansible-playbooks/kubernetes/copy_etcd_cert_file.yml


          cat <<EOF> ~/ansible-playbooks/kubernetes/copy_kubeadm_config_file.yml
          # copy_kubeadm_config_file.yml
          - hosts: MASTER-1
            become: true
            tasks: 
              - name: Copy the ca file to the etcd directory
                copy: 
                  src: ~/configurations/master-1/kubeadm-config.yaml
                  dest: ~/
          EOF

          cat ~/ansible-playbooks/kubernetes/copy_kubeadm_config_file.yml


          mkdir -p files/MASTER-1/etc/kubernetes/pki/etcd
          
          cat <<EOF> ~/ansible-playbooks/kubernetes/copy_kubernetes_certs.yml
          # copy_kubernetes_certs.yml
          ---
          - hosts: MASTER-2, MASTER-3
            become: true
            tasks:
              - name: create directory
                ansible.builtin.file:
                  path: /etc/kubernetes/pki/etcd
                  state: directory
          - hosts: MASTER-1
            become: true
            tasks:
              - name: fetch kubernetes certs to ansbile server
                fetch:
                  src: /etc/kubernetes/{{ item }}
                  dest: ~/files/
                with_items:
                - admin.conf
                - pki/apiserver-etcd-client.crt
                - pki/apiserver-etcd-client.key
                - pki/ca.crt
                - pki/ca.key
                - pki/sa.key
                - pki/sa.pub
                - pki/front-proxy-ca.crt
                - pki/front-proxy-ca.key
                - pki/etcd/ca.crt
                - pki/etcd/ca.key
          - hosts: MASTER-2, MASTER-3
            become: true
            tasks:
              - name: copy kubernetes cerats to MASTER-2,3 server
                copy:
                  src: ~/files/MASTER-1/etc/kubernetes/
                  dest: /etc/kubernetes/
          EOF

          cat ~/ansible-playbooks/kubernetes/copy_kubernetes_certs.yml
    

          cat <<EOF> ~/ansible-playbooks/kubernetes/enable_kubectl_to_master.yml
          # enable_kubectl_to_master.yml
          ---
          - hosts: master
            become: true
            tasks:
              - name: make kube directory
                command: mkdir -p $HOME/.kube
              - name: copy kube config file
                command: sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
              - name: chmod kube config file
                command: sudo chown $(id -u):$(id -g) $HOME/.kube/config           
          EOF

          cat ~/ansible-playbooks/kubernetes/enable_kubectl_to_master.yml


          mkdir -p ~/ansible-playbooks/calico/

          cat <<EOF> ~/ansible-playbooks/calico/copy_calico.yml
          # copy_calico.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
              - name: copy the calico file to the Master-1
                copy: 
                  src: ~/files/BASTION-0/calico/calico.yaml
                  dest: ~/calico.yaml
          EOF

          cat ~/ansible-playbooks/calico/copy_calico.yml


          cat <<EOF> ~/ansible-playbooks/calico/apply_calico_yaml.yml
          # apply_calico_yaml.yml
          - hosts: MASTER-1
            become: true
            tasks: 
              - name: apply calico yaml
                shell: kubectl apply -f calico.yaml
          EOF

          cat ~/ansible-playbooks/calico/apply_calico_yaml.yml


          mkdir -p ~/ansible-playbooks/glusterfs/

          cat <<EOF> ~/ansible-playbooks/glusterfs/start-glusterfs.yml
          # start-glusterfs.yml
          ---
          - hosts: GLUSTERFS-*
            become: true
            tasks:
              - name: enable glusterd service
                command: systemctl enable glusterd.service
              - name: start glusterd service
                command: systemctl start glusterd.service
          EOF

          cat ~/ansible-playbooks/glusterfs/start-glusterfs.yml

          mkdir -p ~/ansible-playbooks/glusterfs/

          cat <<EOF> ~/ansible-playbooks/glusterfs/gluster-peer-probe.yml
          # gluster-peer-probe.yml
          ---
          - hosts: GLUSTERFS-1
            become: true
            tasks:
              - name: Gluster Peer Probe 2
                command: gluster peer probe glusterfs-2
              - name: Gluster Peer Probe 3
                command: gluster peer probe glusterfs-3
          EOF

          cat ~/ansible-playbooks/glusterfs/gluster-peer-probe.yml


          mkdir -p ~/ansible-playbooks/glusterfs

          cat <<EOF> ~/ansible-playbooks/glusterfs/install-heketi-heketi-client.yml
          # install-heketi-heketi-client.yml
          ---
          - hosts: HEKETI-0
            become: true
            tasks:
              - name: install Heketi, Heketi Client
                yum: 
                  name:
                    - heketi
                    - heketi-client
          EOF

          cat ~/ansible-playbooks/glusterfs/install-heketi-heketi-client.yml

          mkdir -p ~/ansible-playbooks/glusterfs

          cat <<EOF> ~/ansible-playbooks/glusterfs/install-glusterfs-client-to-kubernetes.yml
          # install-glusterfs-client-to-kubernetes.yml
          ---
          - hosts: master:worker:router:infra
            become: true
            tasks:
              - name: install glusterfs Client
                yum: 
                  name:
                    - glusterfs-client
          EOF

          cat ~/ansible-playbooks/glusterfs/install-glusterfs-client-to-kubernetes.yml

          mkdir -p ~/ansible-playbooks/glusterfs/

          cat <<EOF> ~/ansible-playbooks/glusterfs/copy_heketi_topology_json.yml
          # copy_heketi_topology_json.yml
          ---
          - hosts: HEKETI-0
            become: true
            tasks:
              - name: copy the heketi topology.json file to the Heketi-0
                copy: 
                  src: ~/configurations/heketi/topology.json
                  dest: /etc/heketi/topology.json
              - name: apply the heketi topology.json file at the Heketi-0 
                command: heketi-cli topology load --user admin --secret hekti_admin_secret --json=/etc/heketi/topology.json
          EOF

          cat ~/ansible-playbooks/glusterfs/copy_heketi_topology_json.yml

          mkdir -p ~/ansible-playbooks/glusterfs/

          cat <<EOF> ~/ansible-playbooks/glusterfs/copy_glusterfs_sc_pvc_files.yml
          # copy_glusterfs_sc_pvc_files.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
              - name: copy glusterfs_sc_pvc_files
                copy: 
                  src: ~/configurations/heketi/{{ item }}
                  dest: ~/glusterfs/
                with_items:
                - gluster-secret.yaml
                - glusterfs-sc.yaml
                - gluster-pvc.yaml
          EOF

          cat ~/ansible-playbooks/glusterfs/copy_glusterfs_sc_pvc_files.yml

