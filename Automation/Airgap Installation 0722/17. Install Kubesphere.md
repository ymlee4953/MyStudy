# **17. Install Kubesphere**

- Air-Gap Environment
- Install Kubesphere on a running kubernetes cluster
- Get the installation from the Bation(ansible) server and resf of packages from nexus server
  - There are 2 yaml files for installation
    - Copy those files to the master-1 node
    - Modify only 1 file (cluster-configuration.yaml) : add etcd cluster information
    - All procedure be executed by ansible playbook
- Run the yaml files and monitoring with log
- Calico Version : v3.1.1

--- 

1. Prepare installation yamls
   
    1.1 Copy and Modify installation yaml files

    - Create the ansible playbook and execute
    - kubesphere version was selected at the Bastion Server

          mkdir -p ~/ansible-playbooks/kubesphere/
          rm -rf ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls-${K8S_CLUSTER_SHORT}.yml
          
          cat <<EOF> ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls.yml
          # copy_kubesphere_install_yamls.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
              - name: create directory
                ansible.builtin.file:
                  path: ~/kubesphere/
                  state: directory                      
          - hosts: MASTER-1
            become: true
            tasks:
              - name: copy kubesphere_install_yamls to MASTER-1 server
                copy:
                  src:  ~/files/BASTION-0/kubesphere/
                  dest: ~/kubesphere/
              - name: insert etcd cluster IPs to cluster-configuration.yaml at MASTER-1 server
                lineinfile:
                  path: ~/kubesphere/cluster-configuration.yaml
                  insertafter: 'endpointIps: localhost'
                  line: "    endpointIps: ${ETCD_1}, ${ETCD_2}, ${ETCD_3}"
              - name: delete old etcd endpointIPs from cluster-configuration.yaml at MASTER-1 server
                lineinfile:
                  path: ~/kubesphere/cluster-configuration.yaml
                  regexp: 'endpointIps: localhost'
                  state: absent
          EOF

          cat ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls.yml
          cp ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls.yml ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls-${K8S_CLUSTER_SHORT}.yml


          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubesphere/copy_kubesphere_install_yamls.yml

2. Prepare Storage Class and PVC with GlusterFS    

    2.1 Prepare to use GluaterFS Cluster

    - Install glusterfs client to all kubernetes nodes and copy required yamls files
     
          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/install-glusterfs-client-to-kubernetes.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/glusterfs/copy_glusterfs_sc_pvc_files.yml

    2.2 Deploy yamls for GlusterFS to K8s Cluster 
    
    - Deploy yamls
    - Set glusterFS as Default Storage Class (to enable Dynamic PVC)
    - Check the pv, pvc results

          ssh ${MASTER_1}  

          kubectl apply -f ~/glusterfs/gluster-secret.yaml
          kubectl apply -f ~/glusterfs/glusterfs-sc.yaml

          kubectl patch storageclass gluster-heketi -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

          kubectl get sc

          kubectl apply -f ~/glusterfs/gluster-pvc.yaml
          kubectl get pv
          kubectl get pvc         

      - Check the Result: (Sample)

            [root@master-1 ~]# kubectl apply -f ~/glusterfs/gluster-secret.yaml
            secret/heketi-secret created
            [root@master-1 ~]# kubectl apply -f ~/glusterfs/glusterfs-sc.yaml
            storageclass.storage.k8s.io/gluster-heketi created
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl patch storageclass gluster-heketi -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
            storageclass.storage.k8s.io/gluster-heketi patched
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl get sc
            NAME                       PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
            gluster-heketi (default)   kubernetes.io/glusterfs   Delete          Immediate           true                   0s
            [root@master-1 ~]#
            [root@master-1 ~]# kubectl apply -f ~/glusterfs/gluster-pvc.yaml
            persistentvolumeclaim/gluster-pvc created
            [root@master-1 ~]#
            ---
            Few seconds later ...
            ---
            [root@master-1 ~]# kubectl get pv
            NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS     REASON   AGE
            pvc-67205a14-4061-4e80-a4b1-0b335c36bf23   1Gi        RWO            Delete           Bound    default/gluster-pvc   gluster-heketi            6s
            [root@master-1 ~]# kubectl get pvc
            NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     AGE
            gluster-pvc   Bound    pvc-67205a14-4061-4e80-a4b1-0b335c36bf23   1Gi        RWO            gluster-heketi   13s
            [root@master-1 ~]#


3. Install Kubesphere

    3.1 Apply installer and configuration yamls

    - at the master-1 server

          kubectl apply -f ~/kubesphere/kubesphere-installer.yaml
          kubectl apply -f ~/kubesphere/cluster-configuration.yaml

    3.2 Monitor the installation with log

    - at the master-1 server

          kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f

      - Check the Result : (Sample)

            PLAY RECAP *********************************************************************
            localhost                  : ok=31   changed=25   unreachable=0    failed=0    skipped=15   rescued=0    ignored=0

            Start installing monitoring
            Start installing multicluster
            Start installing openpitrix
            Start installing network
            **************************************************
            Waiting for all tasks to be completed ...
            task multicluster status is successful  (1/4)
            task network status is successful  (2/4)
            task openpitrix status is successful  (3/4)
            task monitoring status is successful  (4/4)
            **************************************************
            Collecting installation results ...
            #####################################################
            ###              Welcome to KubeSphere!           ###
            #####################################################

            Console: http://10.178.165.169:30880
            Account: admin
            Password: P@88w0rd

            NOTESï¼š
              1. After you log into the console, please check the
                monitoring status of service components in
                "Cluster Management". If any service is not
                ready, please wait patiently until all components
                are up and running.
              2. Please change the default password after login.

            #####################################################
            https://kubesphere.io             2021-07-24 11:11:19
            #####################################################


    3.3 Enable access from public IP (internet)

    - at the master-1 server

          kubectl edit service ks-console -n kubesphere-system

      - Change Value

            spec.type: NodePort --> spec.type: LoadBalancer

    - Check the change result :  

          kubectl get service ks-console -n kubesphere-system

      - Check the result : External-IP should up

            [root@master-1 ~]# kubectl get service ks-console -n kubesphere-system
            NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
            ks-console   LoadBalancer   20.96.212.138   <pending>     80:30880/TCP   15m
            [root@master-1 ~]#

    - Access from the internet browser

          http://xxx.xxx.xxx.xxx:30880               (xxx.xxx.xxx.xxx --> pulbic ip of LB-1)
