2. init 
  -  
    - 
      -  

          cat <<EOF> ~/ansible-playbooks/dirdir/playbookfile.yml
          # playbookfile.yml
          ---
          - hosts: ETCD-1
            become: true
            tasks:
              - name: initialize etcd cluster
                command: ~/etcdadm init
          EOF

          cat ~/ansible-playbooks/dirdir/playbookfile.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/dirdir/playbookfile.yml
          



5.3 Create the first control plane nodes with kubeadm init

At the master-1 Server

Before kubeadm init, check the followings if the VM is restarted

systemctl restart containerd
swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


systemctl daemon-reload
systemctl restart kubelet

systemctl status kubelet -l
Execute kubeadm init

kubeadm init --config kubeadm-config.yaml --upload-certs
After Successful installation, copy the Join tokens key value

# these are look like ...
# for control plane

You can now join any number of the control-plane node running the following command on each as root:

kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
--discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06 \
--control-plane --certificate-key 03796de58479263e9a5554d6e1f520beb48d207d995189d21a3a1478a4f092b5

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!

# for control plane

Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
--discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06
Kubernetes access environment

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Check Kubernetes cluster

kubectl get nodes
kubectl get pods -o wide --all-namespaces
Finalize Control plane with joining Kubernetes Master node 2, 3

6.1 Prepare Certs for Master members

At the master-1

mkdir /root/k8s_certs
cp /etc/kubernetes/pki/apiserver-etcd-client.crt /root/k8s_certs
cp /etc/kubernetes/pki/apiserver-etcd-client.key /root/k8s_certs
cp /etc/kubernetes/pki/ca.crt /root/k8s_certs
cp /etc/kubernetes/pki/ca.key /root/k8s_certs
cp /etc/kubernetes/pki/sa.key /root/k8s_certs
cp /etc/kubernetes/pki/sa.pub /root/k8s_certs
cp /etc/kubernetes/pki/front-proxy-ca.crt /root/k8s_certs
cp /etc/kubernetes/pki/front-proxy-ca.key /root/k8s_certs
cp /etc/kubernetes/pki/etcd/ca.crt /root/k8s_certs/etcd-ca.crt
cp /etc/kubernetes/pki/etcd/ca.key /root/k8s_certs/etcd-ca.key
cp /etc/kubernetes/admin.conf /root/k8s_certs
6.2 Copy and move Certs to Master member servers

Copy cert files from Master_1 Server

At the master-2, 3

sftp root@${MASTER_1A}

get -R /root/k8s_certs
exit
Move cert files to the kubernetes certs path

At the master-2, 3

mkdir -p /etc/kubernetes/pki/etcd
mv /root/k8s_certs/apiserver-etcd-client.crt /etc/kubernetes/pki/
mv /root/k8s_certs/apiserver-etcd-client.key /etc/kubernetes/pki/
mv /root/k8s_certs/ca.crt /etc/kubernetes/pki/
mv /root/k8s_certs/ca.key /etc/kubernetes/pki/
mv /root/k8s_certs/sa.pub /etc/kubernetes/pki/
mv /root/k8s_certs/sa.key /etc/kubernetes/pki/
mv /root/k8s_certs/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/k8s_certs/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/k8s_certs/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/k8s_certs/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/k8s_certs/admin.conf /etc/kubernetes/admin.conf
6.3. Join to the Kubeadm Join

At the master-2, 3

kubeadm join 10.168.40.37:6443 --token v4hs9a.hstnvaq93w73pvo9 \
  --discovery-token-ca-cert-hash sha256:b6af0746091b649fd69ec64a4a2b9082f140e201304f6c7fdf9b2710e41a523e \
  --control-plane --certificate-key 14c77e611f4f15eec09194cafaa0994b5c0281a76bef0be4b09ea268afc4e3b8
Kubernetes access environment

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Kubernetes Worker node Join

7.1 Join worker nodes

Worker node 1, 2, 3

kubeadm join 10.168.40.37:6443 --token v4hs9a.hstnvaq93w73pvo9 \
      --discovery-token-ca-cert-hash sha256:b6af0746091b649fd69ec64a4a2b9082f140e201304f6c7fdf9b2710e41a523e
Set node Label

8.1 Set node label for the advanced tasks

At the master-1

export CLUSTER_DOMAIN=a.k8s.demo.ymlee  

cat <<EOF> labeltest.txt
kubectl label nodes master-1.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes master-2.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes master-3.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes ingress-1.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes ingress-2.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes ingress-3.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes worker-1.${CLUSTER_DOMAIN} node-type=app
kubectl label nodes worker-2.${CLUSTER_DOMAIN} node-type=app
kubectl label nodes worker-3.${CLUSTER_DOMAIN} node-type=app
EOF

cat labeltest.txt
rm labeltest.txt
Install CNI : Calico

CNI : Calico
Typha 적용
9.1 Download and copy the yaml file for the Calico Typha

Download the yaml File

At the Bastion server

mkdir -p ~/calico/typha

curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml

mv ./calico.yaml  ~/calico/typha/  
or

mkdir -p ~/calico

curl https://docs.projectcalico.org/manifests/calico.yaml -o calico.yaml
curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico-typha.yaml

mv ./calico*.yaml  ~/calico
Copy the yaml File

at Master node 1

sftp root@${BASTION_0}

get calico/calico.yaml
get calico/calico-typha.yaml
exit
9.2 Set typha configuration : set "node-type: router" at the nodeSelector

at Master node 1

vi calico-typha.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
...
  template:
    ...
    spec:
      nodeSelector:
        kubernetes.io/os: linux
        node-type: router
      ...
9.3 Install Calico

at Master node 1

kubectl apply -f calico.yaml

or 

kubectl apply -f calico-typha.yaml
9.4 Check the Result

at Master node 1

kubectl get nodes
# 모든 노드의 상태가 Ready 로 변경


kubectl get pods --all-namespaces
# Calico POD 정상 기동
# coredns 상태 Ready 로 변경          