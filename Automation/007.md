2. init 
  -  
    - 
      -  

          cat <<EOF> ~/ansible-playbooks/kubernetes/kubeadm_init.yml
          # playbookfile.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
<<<<<<< Updated upstream
              - name: initialize kubernetes cluster
                command: kubeadm init --config kubeadm-config.yaml --upload-certs
=======
              - name: initialize etcd cluster
                command: ~/kubeadm init --config kubeadm-config.yaml --upload-certs >> kubeadm_init_log.txt
>>>>>>> Stashed changes
          EOF

          cat ~/ansible-playbooks/kubernetes/kubeadm_init.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/kubeadm_init.yml
<<<<<<< Updated upstream
          
=======
>>>>>>> Stashed changes

2. set_kubectl 
  -  
    - 
      -  

          cat <<EOF> ~/ansible-playbooks/kubernetes/set_kubectl.yml
          # set_kubectl.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
              - name: make .kube directory
                command: mkdir -p $HOME/.kube                
              - name: copy the config file
                command: sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
              - name: make executabe
                command: sudo chown $(id -u):$(id -g) $HOME/.kube/config
          EOF

          cat ~/ansible-playbooks/kubernetes/set_kubectl.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/set_kubectl.yml
          

3. copy control plan certs 
  -  
    - 
      -  
          
          cat <<EOF> ~/ansible-playbooks/kubernetes/copy_control_plane_certs.yml
          # copy_control_plane_certs.yml
          ---
          - hosts: MASTER-1
            become: true
            tasks:
              - name: Create a directory if it does not exist
                ansible.builtin.file:
                  path: /root/files/MASTER-1/k8s_certs
                  state: directory
              - name: Fetch the control plane certs file from the Bastion_0
                fetch: 
                  src: /root/etcdadm/{{ item }}
                  dest: /root/files/MASTER-1/k8s_certs/
                with_items:
                - etcdadm
                - etcd-v3.4.9-linux-amd64.tar.gz  
          - hosts: etcd
            become: true
            tasks:           
              - name: Copy the file to etcd
                copy:
                  src:  ~/files/BASTION-0/root/etcdadm/etcdadm
                  dest: ~/
          - hosts: etcd
            become: true
            tasks:           
              - name: chmod of etcdadm
                command: chmod 755 ~/etcdadm
          - hosts: etcd
            become: true
            tasks:           
              - name: Copy the tar file to etcd v3.4.9
                copy:
                  src:  ~/files/BASTION-0/root/etcdadm/etcd-v3.4.9-linux-amd64.tar.gz
                  dest: /var/cache/etcdadm/etcd/v3.4.9/
          EOF

          cat ~/ansible-playbooks/kubernetes/copy_control_plane_certs.yml

          ansible-playbook -i k8s-cluster-hosts ~/ansible-playbooks/kubernetes/copy_control_plane_certs.yml



              - name: make directory at loadbalancer servers
                file:                  
                  path: "{{ item }}"
                  state: directory
                loop:            
                  - /opt/haproxy-1.7.8/
                  - /etc/haproxy/
                  - /var/lib/haproxy/      



kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
--discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06 \
--control-plane --certificate-key 03796de58479263e9a5554d6e1f520beb48d207d995189d21a3a1478a4f092b5

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!

# for control plane

Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.168.40.39:6443 --token eg8ro5.o0lc5jsazh232pge \
--discovery-token-ca-cert-hash sha256:977a671d5874e657410e687023d1bcf20b154d31b7bd25ec026be8aed90a8c06
Kubernetes access environment

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Check Kubernetes cluster

kubectl get nodes
kubectl get pods -o wide --all-namespaces
Finalize Control plane with joining Kubernetes Master node 2, 3

6.1 Prepare Certs for Master members

At the master-1

mkdir /root/k8s_certs
cp /etc/kubernetes/pki/apiserver-etcd-client.crt /root/k8s_certs
cp /etc/kubernetes/pki/apiserver-etcd-client.key /root/k8s_certs
cp /etc/kubernetes/pki/ca.crt /root/k8s_certs
cp /etc/kubernetes/pki/ca.key /root/k8s_certs
cp /etc/kubernetes/pki/sa.key /root/k8s_certs
cp /etc/kubernetes/pki/sa.pub /root/k8s_certs
cp /etc/kubernetes/pki/front-proxy-ca.crt /root/k8s_certs
cp /etc/kubernetes/pki/front-proxy-ca.key /root/k8s_certs
cp /etc/kubernetes/pki/etcd/ca.crt /root/k8s_certs/etcd-ca.crt
cp /etc/kubernetes/pki/etcd/ca.key /root/k8s_certs/etcd-ca.key
cp /etc/kubernetes/admin.conf /root/k8s_certs
6.2 Copy and move Certs to Master member servers

Copy cert files from Master_1 Server

At the master-2, 3

sftp root@${MASTER_1A}

get -R /root/k8s_certs
exit
Move cert files to the kubernetes certs path

At the master-2, 3

mkdir -p /etc/kubernetes/pki/etcd
mv /root/k8s_certs/apiserver-etcd-client.crt /etc/kubernetes/pki/
mv /root/k8s_certs/apiserver-etcd-client.key /etc/kubernetes/pki/
mv /root/k8s_certs/ca.crt /etc/kubernetes/pki/
mv /root/k8s_certs/ca.key /etc/kubernetes/pki/
mv /root/k8s_certs/sa.pub /etc/kubernetes/pki/
mv /root/k8s_certs/sa.key /etc/kubernetes/pki/
mv /root/k8s_certs/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/k8s_certs/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/k8s_certs/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/k8s_certs/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/k8s_certs/admin.conf /etc/kubernetes/admin.conf
6.3. Join to the Kubeadm Join

At the master-2, 3

kubeadm join 10.168.40.37:6443 --token v4hs9a.hstnvaq93w73pvo9 \
  --discovery-token-ca-cert-hash sha256:b6af0746091b649fd69ec64a4a2b9082f140e201304f6c7fdf9b2710e41a523e \
  --control-plane --certificate-key 14c77e611f4f15eec09194cafaa0994b5c0281a76bef0be4b09ea268afc4e3b8
Kubernetes access environment

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Kubernetes Worker node Join

7.1 Join worker nodes

Worker node 1, 2, 3

kubeadm join 10.168.40.37:6443 --token v4hs9a.hstnvaq93w73pvo9 \
      --discovery-token-ca-cert-hash sha256:b6af0746091b649fd69ec64a4a2b9082f140e201304f6c7fdf9b2710e41a523e
Set node Label

8.1 Set node label for the advanced tasks

At the master-1

export CLUSTER_DOMAIN=a.k8s.demo.ymlee  

cat <<EOF> labeltest.txt
kubectl label nodes master-1.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes master-2.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes master-3.${CLUSTER_DOMAIN} node-type=master
kubectl label nodes ingress-1.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes ingress-2.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes ingress-3.${CLUSTER_DOMAIN} node-type=router
kubectl label nodes worker-1.${CLUSTER_DOMAIN} node-type=app
kubectl label nodes worker-2.${CLUSTER_DOMAIN} node-type=app
kubectl label nodes worker-3.${CLUSTER_DOMAIN} node-type=app
EOF

cat labeltest.txt
rm labeltest.txt
Install CNI : Calico

CNI : Calico
Typha 적용
9.1 Download and copy the yaml file for the Calico Typha

Download the yaml File

At the Bastion server

mkdir -p ~/calico/typha

curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml

mv ./calico.yaml  ~/calico/typha/  
or

mkdir -p ~/calico

curl https://docs.projectcalico.org/manifests/calico.yaml -o calico.yaml
curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico-typha.yaml

mv ./calico*.yaml  ~/calico
Copy the yaml File

at Master node 1

sftp root@${BASTION_0}

get calico/calico.yaml
get calico/calico-typha.yaml
exit
9.2 Set typha configuration : set "node-type: router" at the nodeSelector

at Master node 1

vi calico-typha.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
...
  template:
    ...
    spec:
      nodeSelector:
        kubernetes.io/os: linux
        node-type: router
      ...
9.3 Install Calico

at Master node 1

kubectl apply -f calico.yaml

or 

kubectl apply -f calico-typha.yaml
9.4 Check the Result

at Master node 1

kubectl get nodes
# 모든 노드의 상태가 Ready 로 변경


kubectl get pods --all-namespaces
# Calico POD 정상 기동
# coredns 상태 Ready 로 변경          